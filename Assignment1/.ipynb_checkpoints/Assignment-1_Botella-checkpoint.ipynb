{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS171 - Winter 2020 - Assignment 1\n",
    "### Instructor: Vagelis Papalexakis\n",
    "### TA: Yorgos Tsitsikas\n",
    "\n",
    "In this first assignment you will explore a dataset, visualizing the dataset in various ways, and doing a preliminary analysis on the data. \n",
    "\n",
    "For this assignment we are going to use the functionality of Pandas (the library, *not* the unbearably cute animal): https://pandas.pydata.org/ in order to manipulate datasets.\n",
    "In addition to Pandas, we are going to use Matplotlib (https://matplotlib.org/) and Numpy (http://www.numpy.org/) and you may also find Seaborn (https://seaborn.pydata.org/) useful for some data visualization.\n",
    "\n",
    "Unless you are explicitly asked to *implement* a particular functionality, you may assume that you may use an existing implementation from the libraries above (or some other library that you may find, as long as you *document* it).\n",
    "\n",
    "Before you start, make sure you have installed all those packages in your local Jupyter instance, as follows:\n",
    "\n",
    "conda install numpy pandas matplotlib seaborn\n",
    "\n",
    "## Academic Integrity\n",
    "Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TAs, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources, and if the help is significant, the appropriate grade reduction will be applied. If you fail to do so, the instructor and the TAs are obligated to take the appropriate actions outlined at http://conduct.ucr.edu/policies/academicintegrity.html . Please read carefully the UCR academic integrity policies included in the link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn\n",
    "import pandas as pd, matplotlib.pyplot as plt, seaborn as sb, numpy as np\n",
    "#make sure you import here everything else you may need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0: Getting real data [0%] \n",
    "\n",
    "In this assignment you are going to use data from the UCI Machine Learning repository ( https://archive.ics.uci.edu/ml/index.php ). In particular, you are going to use the famous Iris dataset: https://archive.ics.uci.edu/ml/datasets/Iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label']\n",
    "data = pd.read_csv('iris.data', \n",
    "                   names = data_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Data Visualization [20%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1a: Scatterplots [10%]\n",
    "1. Plot the scatterplot of all pairs of features and color the points by class label [5%]\n",
    "2. Which pair of features is (visually) the most correlated?  [2.5%]\n",
    "3. Can you think of a reason why looking at this plot would be useful in a task where we would have to classify flowers by label? [2.5%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- \n",
    "#USING PANDAS (can't add colors)\n",
    "scat_mat = pd.plotting.scatter_matrix(data, alpha=0.5)\n",
    "\n",
    "\n",
    "#USING SEABORN (pairplot function)\n",
    "sb.set()\n",
    "data_sb = sb.load_dataset('iris')\n",
    "sb.pairplot(data_sb, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here:\n",
    "2. petal_width and petal_lenght are the features that are visually the most correlated (linearly correlated). \n",
    "3. Looking at the plot can help us spot the correlated features (as in question 1)  and give us an idea about what can be the important features to classify flowers by lable. We can see that smaller petal_lenght and smaller petal_width would/could leed to setosa flower. Without seeing an image of the flower we can imagine the setosa being the flower with small petals and a long sepal, virginica big petals smaller sepal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b: Boxplot and Histogram [10%]\n",
    "\n",
    "1. Plot the boxplot for each feature of the dataset (you can put all boxplots on a single figure) [4%]\n",
    "2. Plot the histogram only for petal length [4%]\n",
    "3. Does the histogram for petal length give more information than the boxplot? If so, what information? [2%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- Plot the boxplot for each feature of the dataset\n",
    "box_p_lenght = data['petal_length']\n",
    "box_p_width = data['petal_width'] \n",
    "box_s_lenght = data['sepal_length']\n",
    "box_s_width = data['sepal_width'] \n",
    "\n",
    "fig, axes = plt.subplots(2,2) #add figure of 4 (2x2) subplots to plot the boxplots\n",
    "\n",
    "axes[0,0].boxplot(box_p_lenght)\n",
    "axes[0,0].set_title('petal_length')\n",
    "\n",
    "axes[0,1].boxplot(box_p_width)\n",
    "axes[0,1].set_title('petal_width')\n",
    "\n",
    "axes[1,0].boxplot(box_s_lenght)\n",
    "axes[1,0].set_title('sepal_length')\n",
    "\n",
    "axes[1,1].boxplot(box_p_width)\n",
    "axes[1,1].set_title('sepal_width')\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- Plot all boxplots on a single figure\n",
    "data_boxplot = [data['petal_length'], data['petal_width'], data['sepal_length'], data['sepal_width']] \n",
    "\n",
    "plt.subplots()\n",
    "plt.boxplot(data_boxplot)\n",
    "\n",
    "plt.xticks([1, 2, 3, 4], ['petal_length', 'petal_width', 'sepal_length', 'sepal_width'])\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2- Plot the histogram only for petal length \n",
    "plt.hist(data['petal_length'])\n",
    "display()\n",
    "#data['petal_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the histogram for petal length give more information than the boxplot? If so, what information? [2%]\n",
    "Your answer here:\n",
    "\n",
    "3. Yes the histogram for petal length is giving more information than the boxplot because it also shows the frequency distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Distance computation [40%]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2a: Implement the Lp distance function [20%]\n",
    "1. Write code that implements the Lp distance function between two data points as we saw it in class [15%]\n",
    "2. Verify that it is correct by comparing it for p=2 against an existing implementation in Numpy for the two selected data points below. Note that the difference of the distances may not be exactly 0 due to numerical precision issues. [5%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lp.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1-Write code that implements the Lp distance function between two data points as we saw it in class [15%]\n",
    "### ELIF function\n",
    "'''\n",
    "def lp(p, X, Y):\n",
    "    if p==1:\n",
    "        dist = sum([abs((x - y)) ** p for x, y in zip(X, Y)])**1/p\n",
    "        print(\"Manhattan distance 'L1' from x to y: \",dist)\n",
    "    elif p==2:\n",
    "        dist = sum([abs((x - y)) ** p for x, y in zip(X, Y)])**1/p\n",
    "        print(\"Euclidean distance 'L2' from x to y: \",dist)\n",
    "    else:\n",
    "        print(\"Choose L1 or L2 by key in 1 or 2\")\n",
    "    return\n",
    "'''           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-Write code that implements the Lp distance function between two data points as we saw it in class [15%]\n",
    "def lp(p, X, Y):\n",
    "    dist = (sum([abs((x - y)) ** p for x, y in zip(X, Y)]))**(1/p)\n",
    "    print(\"L\"+str(p)+\" Distance from x to y: \",dist)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data.loc[3])[0:4]\n",
    "Y = list(data.loc[5])[0:4]\n",
    "lp(2, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify that it is correct by comparing it for p=2 against an existing implementation in Numpy for the two selected data points below.\n",
    "import math\n",
    "from numpy import linalg\n",
    "\n",
    "def compare(X,Y):\n",
    "    lp_ = lp(2,X,Y)\n",
    "    lp_np = np.linalg.norm(np.subtract(X,Y))\n",
    "            #diff = (np.diff([X, Y]) ** 2).sum()    WORKS ONLY WITH 2D points\n",
    "            #lp_np = math.sqrt(diff)\n",
    "    print(\"L2 with Numpy = \" + str(lp_np) + \"\\nDifference of the distance : \" + str(lp_ - lp_np))\n",
    "    return\n",
    "#Note that the difference of the distances may not be exactly 0 due to numerical precision issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2b: Compute the distance matrix between all data points [20%]\n",
    "1. Compute an $N\\times N$ distance matrix between all data points (where $N$ is the number of data points) [5%]\n",
    "2. Plot the above matrix and include a colorbar. [5%]\n",
    "3. What is the minimum number of distance computations that you can do in order to populate every value of this matrix? (note: it is OK if in the first two questions you do all the $N^2$ computations) [5%]\n",
    "4. Note that the data points in your dataset are sorted by class. What do you observe in the distance matrix? [5%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Compute an N*N distance matrix between all data points (where N is the number of data points) [5%]\n",
    "\n",
    "#Initialize a 2D matrix with 0\n",
    "n = len(data)\n",
    "matrix = np.zeros((n,n))\n",
    "matrix\n",
    "#feed the matrix with a double for loop\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "            d1 = list(data.loc[i])[0:4] #remove label at [5] (species) and cast into a list of the 5 elements\n",
    "            d2 = list(data.loc[j])[0:4] \n",
    "            #print(d1,d2)\n",
    "            d1f = [float(i) for i in d1]\n",
    "            d2f = [float(i) for i in d2]\n",
    "            matrix[i][j]=lp(2, d1f, d2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the matrix\n",
    "plt.matshow(matrix)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer here:\n",
    "#### 3. What is the minimum number of distance computations that you can do in order to populate every value of this matrix?\n",
    "The minimum number of computation is N*N/2 because the matrix is symetric. That means we need only one half of the sqare matrix, one triangle, to know all the distance.\n",
    "#### 4. Note that the data points in your dataset are sorted by class. What do you observe in the distance matrix? [5%]\n",
    "We can visually distinguish the 3 different classes, by spoting 3 different squares. Each of them representing one of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Data Sampling [40%]\n",
    "\n",
    "Sometimes datasets are too big, or come in a streaming fashion, and it is impossible for us to process every single data point, so we have to resort to sampling methods. In this question, you will implement the popular \"reservoir sampling\" method, which is mostly used to obtain a uniform random sample of a data stream. Subsequently, you will experiment with sampling directly all the data and conducting stratified sampling (by class label) and observe the results in the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a: Reservoir Sampling [20%]\n",
    "1. Implement reservoir sampling as we saw it in class. Create a 'reservoir_sampling' function because it will be useful for the next question. [15%]\n",
    "2. Run reservoir sampling with reservoir size $M = 15$ and plot the histogram of the petal length feature for the sampled dataset [5%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-Implement reservoir sampling as we saw it in class. [15%]\n",
    "import random\n",
    "def reservoir_sampling(stream,M):\n",
    "    res = [0]*M\n",
    "    \n",
    "    for i in range(M):\n",
    "        res[i] = stream.loc[i]\n",
    "    \n",
    "    while(i < len(stream)):\n",
    "        j = random.randrange(i+1)\n",
    "        \n",
    "        if(j < M):\n",
    "            res[j] = stream.loc[i] \n",
    "            i+=1\n",
    "    return pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2- Run reservoir sampling with reservoir size  ð‘€=15  and plot the histogram of the petal length feature for the sampled dataset\n",
    "res_sample = reservoir_sampling(data, M=15)\n",
    "res_histogram = res_sample['petal_length'].plot(kind=\"hist\", grid=True, title=\"Petal Lengths Reservoir Sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b: Stratified Sampling [20%]\n",
    "1. Implement stratified sampling by class label, and within each stratum use the reservoir sampling function you implemented. [15%]\n",
    "2. Run your stratified sampler with $M=5$ samples per class (so that we have 15 data points in total) and plot the histogram of the petal length feature for the sampled dataset [2.5%]\n",
    "3. Do you observe any difference between the stratified and the non-stratified histograms? Which one resembles the original petal length distribution more closely? In order to answer this question you may want to run both sampling procedures a few times and observe which one gives a more accurate result on average. [2.5%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- Implement stratified sampling by class label\n",
    "def stratified_sampling(stream, M):\n",
    "    #define the 3 classes\n",
    "    setosas = []\n",
    "    versicolors = []\n",
    "    virginicas = []\n",
    "    \n",
    "    #Append each data point to its corresponding class\n",
    "    for i in range(len(stream)):\n",
    "        item = stream.loc[i]\n",
    "        class_label = item[\"label\"]\n",
    "        \n",
    "        if class_label == \"Iris-setosa\":\n",
    "            setosas.append(item)\n",
    "        elif class_label == \"Iris-versicolor\":\n",
    "            versicolors.append(item)\n",
    "        elif class_label == \"Iris-virginica\":\n",
    "            virginicas.append(item)\n",
    "    \n",
    "    #Convert in df and reseet index \n",
    "    setosas = pd.DataFrame(setosas).reset_index(drop=True)\n",
    "    versicolors = pd.DataFrame(versicolors).reset_index(drop=True)\n",
    "    virginicas = pd.DataFrame(virginicas).reset_index(drop=True)\n",
    "    \n",
    "    # Reservoir sampling and merging\n",
    "    res_setosas = reservoir_sampling(setosas, M).reset_index(drop=True)\n",
    "    res_versicolors = reservoir_sampling(versicolors, M).reset_index(drop=True)\n",
    "    res_virginicas = reservoir_sampling(virginicas, M).reset_index(drop=True)\n",
    "    \n",
    "    res_sample = pd.concat([res_setosas, res_versicolors, res_virginicas]).reset_index(drop=True)\n",
    "    \n",
    "    return res_sample\n",
    "\n",
    "strat_sample = stratified_sampling(data, 5)\n",
    "strat_histogram = strat_sample[\"petal_length\"].plot(kind=\"hist\", grid=True, title=\"Petal Lengths Stratified Sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Do you observe any difference between the stratified and the non-stratified histograms? Which one resembles the original petal length distribution more closely? In order to answer this question you may want to run both sampling procedures a few times and observe which one gives a more accurate result on average. [2.5%]\n",
    "\n",
    "The plots look similar but the stratified one is more stable, it changed less compare to the non-stratified histogram. The stratified histogram is closer to the original histogram of petal_length. It might be more accurate because we are using the 3 classes for the sampling. \n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
